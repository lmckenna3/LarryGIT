{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Basics of Machine Learning<p>\n",
    "$LWM^3$ 27 Feb 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Stratigraphy of AI\n",
    "1. Machine Learning\n",
    "    * Supervised Learning\n",
    "    * Unsupervised Learning\n",
    "    * Reinforced Learning\n",
    "2. Deep Learning\n",
    "    * Aritfical Neural Nets\n",
    "    * Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Categories of ML\n",
    "1. Supervised learning<p>\n",
    "Write code that predicts an outcome (this is spam email, this is not; this combination of factors predicts student A will earn a C in Calc I; this student an A), based on a training set of data. The outcome of the training set is known *a priori*; no exploration of the data for a signal is needed.\n",
    "    * Labeled Data\n",
    "    * Direct feedback\n",
    "    * Classify new data with label\n",
    "    \n",
    "   Sub-categories\n",
    "   * Classification: discrete output\n",
    "   * Regression: continuous output\n",
    "       * Predictor variables $\\equiv  $ *feature variable*\n",
    "       * Response variables $\\equiv $ *target variables*\n",
    "2. Reinforcement learning<p>\n",
    "Write code (called an \"agent\") that iteratively learns vs. trial an error to associate certain states of an environemnt (a chess boars, a traffic pattern, a series of trades on a market) with a feedback signal (the reward) which occur as a result of an action the agent takes. The reward is known *a priori*; one either wins or loses a chess game, traffic delays of a certain magnitude emerge, wins or losses occurr.\n",
    "    * Decision process\n",
    "    * Reward sytem\n",
    "    * Learn(ed) series of actions\n",
    "3. Unsupervised learning\n",
    "    * No Labels\n",
    "    * No Feedback\n",
    "    * Find hidden structure in data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning: an example\n",
    "## The Perceptron\n",
    "As noted by wiki...\n",
    "\"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.[1] It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\"<p>\n",
    "Algorithm invented in 1958 at the Cornell Aeronautical Laboratory by Frank Rosenblatt, funded by the United States Office of Naval Research. The perceptron is limited (capable of linear discrimination only) and is no longer used in practice. But its elegant simplicity and mathematical tractability make it \"hand-doable\" and an excellent way of learning how machines learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Flow\n",
    "0. Assemble training data\n",
    "    * Data can have any number of *features*, which we'll index by i.\n",
    "    * Data can have any number of observations, or points, which we'll index by j.\n",
    "    * Data must belong to one of two *classes,* which we'll designate as -1 and 1.\n",
    "    * Randomize the order of the observation<p>\n",
    "\n",
    "1. Create a *Feature 0* for each observation\n",
    "    * This is a *bias* term, esentially a constant\n",
    "    * $F_{0,j}=1$\n",
    "    * Or, in English, set Feature 0 equal to 1 for all observations $j=1, 2, 3...j_{max}$.<p>\n",
    "\n",
    "2. Assign initial *weights,* one for each feature\n",
    "    * Typically $w_i=N(0,small)$\n",
    "    * Or, in English, set weight for each feature $i=0, 1, 2...i_{max}$ to a random number drawn from a normal distribution with mean 0 and a small standard deviation, 0.01 to 0.1.<p>\n",
    "    \n",
    "3. Repeat the following loop until some \"end\" condition is reached, running through each data point in turn\n",
    "    1. Locate the data point\n",
    "        * find \n",
    "\\begin{align}\n",
    "S_j=\\sum w_i \\cdot F_{i,j}\n",
    "\\end{align}\n",
    "        * Or, in English, find for each observation j: $S_j=w_0 \\cdot F_{0,j}+w_1 \\cdot F_{1,j}+w_2 \\cdot F_{2,j}+... w_{i_{max}} \\cdot F_{{i_{max}},j}$\n",
    "        * This step converts the coordnates of a point to a \"lever arm,\" which will help tilt the next generation of weights in a way that will tume them to seperate the groups.\n",
    "    2. Predict which group the observation belongs to, by using a threshold function based on the value of S.\n",
    "        $$\n",
    "\\begin{align}\n",
    " P_j=\\begin{cases}\n",
    "    1, & \\text{if $S_j>0$}.\\\\\n",
    "    -1, & \\text{if $S_j \\leq 0$}.\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "        $$\n",
    "    3. Change the weights, based on whether the observation was correctly classified.\n",
    "        * If the *predicted* class is correct, weights are good, do nothing\n",
    "        * But is the *predicted* class is wrong, change the weights in the direction that will improve next prediction\n",
    "        * This is the AI, the learning step. \n",
    "        * $ \\Delta w_i=r \\cdot (C_j-P_j) \\cdot F_{i,j}$\n",
    "        * $r$ is the *learning rate*, where $0<r<1$. The rate has to be large enough to allow the loop to end, but small enough to prevent wild oscillations. Start with $r=0.1$\n",
    "        * Let's work through this by hand. Assume that observation 2 is classed as $C_2=-1$, and imagine two outcomes for $P_2$:\n",
    "        \\begin{align}\n",
    " P_2=\n",
    " \\begin{cases}\n",
    "    1 \\text{ (prediction incorrect)}, & \\Delta w_i=r \\cdot (-1-1) \\cdot F_{i,j}&=r \\cdot (2) \\cdot F_{i,j}= 2rF_{i,j}\\\\\n",
    "    -1 \\text{ (prediction correct)}, & \\Delta w_i=r \\cdot (-1-(1)) \\cdot F_{i,j}&=r \\cdot (0) \\cdot F_{i,j}= 0 \n",
    "  \\end{cases}\n",
    "       \\end{align}\n",
    "        * If the weights give a correct prediction, no change. But if they don't give the correct prediction, the weights are changed, and in a direction likely to lead to better prediction, by a proportion of $2r$ of the Feature value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's practice with some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 2 7 1\n",
      "3 1 4 7 1\n",
      "5 1 3 6 1\n",
      "7 1 2 5 1\n",
      "2 1 6 5 -1\n",
      "4 1 5 4 -1\n",
      "6 1 4 3 -1\n",
      "8 1 6 3 -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1de743be208>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#steps 1 and 2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# We'll create an array of the data we need...thi is NOT the best way to organize the data, but it works.\n",
    "# Columns are analysis order (see below), Feature 0 = 1 (always), Feature 1 (x-value), Feture 2(y value), Group \n",
    "df=np.array([[1,1,2,7,1], [3,1,4,7,1], [5,1,3,6,1], [7,1,2,5,1], [2,1,6,5,-1], [4,1,5,4,-1], [6,1,4,3,-1], [8,1,6,3,-1]])\n",
    "for line in df:\n",
    "    print(*line)\n",
    "# plot data\n",
    "plt.scatter(df[0:4,2], df[0:4,3],\n",
    "            color='red', marker='o', label='Group 1')\n",
    "plt.scatter(df[4:8,2], df[4:8,3],\n",
    "            color='blue', marker='s', label='Group -1')\n",
    "plt.xlim(0.,8.)\n",
    "plt.ylim(0.,8.)\n",
    "#plt.axes().set_aspect('equal')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 2 7 1\n",
      "2 1 6 5 -1\n",
      "3 1 4 7 1\n",
      "4 1 5 4 -1\n",
      "5 1 3 6 1\n",
      "6 1 4 3 -1\n",
      "7 1 2 5 1\n",
      "8 1 6 3 -1\n"
     ]
    }
   ],
   "source": [
    "# Ideally one interleaves the classes. let's sort df by analysis order (coulumn 1) \n",
    "df=df[df[:,0].argsort()]\n",
    "for line in df:\n",
    "    print(*line)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we'll find it helpful to keep track of the weights and the predicted class membership. Pre-define an array to hold the weights, which change on each update\n",
    "# to do tis we'll need the shape of the data array.\n",
    "data_shape=np.shape(df)\n",
    "data_shape\n",
    "# Create an empty array of same number of rows and columns; we'll go through the array at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01749765,  0.0034268 ,  0.01153036])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Assign weights\n",
    "rgen = np.random.RandomState(100)# Sets rng to a fixed seed so we all get the same values\n",
    "weights=rgen.normal(loc=0.0, scale=0.01, size=3)# mean, std, number of values\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07006845951545815"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find S1: w_0 *F_{0,1}+w_1 *F_{1,1}+w_2*F_{2,1}\n",
    "S1=sum(weights*df[0,1:4])\n",
    "S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put S1 into the Prediction engine\n",
    "P1=np.sign(S1)# the sign(x) command returns 1 for x>0, 0 for x=0, -1 for x<0. A nice way to evalaute S\n",
    "P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the weights\n",
    "r=0.1\n",
    "deltaw0=r*(df[0,4]-P1)*df[0,1] # recall that Python's really awesome indexing starts with 0. So\n",
    "# df[0,3] is the first row, fourth column of the area...the class of the first observation\n",
    "deltaw0 # note that the weight change is 0, as the prediction is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update weight_1 and weight_2; better, calculate all delta weights in one command...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat for observation 5, the first observation classified as Group \"-1\".\n",
    "use the subscript 2 in your calculations. Do all steps in this cell.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
